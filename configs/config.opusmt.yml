####
# Example of a production config
# Change language pair, experiment name, datasets and other settings if needed
# Training low resource languages might require more tuning of pipeline/training/configs
###


experiment:
  name: opusmt
  src: en
  trg: fi
  src_three_letter: eng
  trg_three_letter: fin

  #OPUS models are not ensembled, they have different vocabs anyway
  teacher-ensemble: 1

  #TODO: change this to be the identifier of the model to download
  opusmt-teacher: True
  #URL to the OPUS-MT model to use as backward model
  opusmt-backward: "https://object.pouta.csc.fi/Tatoeba-MT-models/fin-eng/opusTCv20210807+bt-2021-08-25.zip" 

  # path to a pretrained backward model (optional)
  backward-model: ""
  
  # path to a pretrained vocabulary (optional)
  vocab: "/scratch/project_2006944/tommi/firefox-translations-data/models/en-fi/opusmt/opusTCv20210807+bt-2021-09-01/opusTCv20210807+bt.spm32k-spm32k.vocab.yml"

  # OPUS-MT models are trained with different vocabs
  backward-vocab: "/scratch/project_2006944/tommi/firefox-translations-data/models/fi-en/opusmt/opusTCv20210807+bt-2021-08-25/source.spm"

  # limits per downloaded dataset
  mono-max-sentences-src: 100000000
  mono-max-sentences-trg: 20000000

  # split corpus to parallelize translation
  split-length: 2000000
  
  best-model: perplexity
  bicleaner:
    default-threshold: 0.5
    dataset-thresholds:
      tc_Tatoeba-train-v2021-08-07.eng.fin: 0.5

#marian-args:
#  decoding-teacher:
    # 2080ti or newer
#    precision: float16

#TODO: extract this info straight from the OPUS model yml info file
datasets:
  # parallel training corpus
  train:
    - tc_Tatoeba-Challenge-v2021-08-07 
  # datasets to merge for validation while training
  devtest:
    - tc_Tatoeba-Challenge-v2021-08-07
  # datasets for evaluation
  test:
    - tc_Tatoeba-Challenge-v2021-08-07
